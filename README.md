# Appearance-as-Reliable-Evidence-implementation
The implementation of the paper: Appearance as Reliable Evidence: Reconciling Appearance and Generative Priors for Monocular Motion Estimation

# installation

You can clone the repo by following:
```bash
git clone https://github.com/Zipei-Chen/Appearance-as-Reliable-Evidence-implementation.git
```

Then you create the conda environment by:

```bash
conda env create -f environment.yml
conda activate ARE
```

# Data prepocess 

Firstly, you should download the Prox dataset from the <a href="https://prox.is.tue.mpg.de/" target="_blank">official</a>
and download the other necessary data of prox from <a href="https://drive.google.com/file/d/1NY22JwWsyaGudhWnWYnVgeIYQtlZyZWQ/view" target="_blank">here</a> follow <a href="https://github.com/sanweiliti/RoHM" target="_blank">RoHM</a>

Besides, you should download the SMPL-X model from: <a href="https://smpl-x.is.tue.mpg.de/index.html" target="_blank">SMPL-X</a> and remove the Chumpy object following <a href="https://github.com/vchoutas/smplx/tree/main/tools" target="_blank">this</a>.

Then, some necessary intermediate results can be generated by the following scripts.

Extract the init pose info from the results of the off-the-shelf motion estimation method (here we use the public results of RoHM as the init pose):
```bash
python ./Data_preprocess/0_Init_pose_info_extract_prox.py
```

Predict the coarse human mask by off-the-shelf DeeplabV3:
```bash
python ./Data_preprocess/1_Semantic_segmentation_deeplabv3_prox.py
```
mode estimation:
```bash
python ./Data_preprocess/2_Scene_mode_estimation_prox.py
```
Extract the motion depth from the initial pose parameters:
```bash
python ./Data_preprocess/3_Motion_depth_generation_prox.py
```

Predict the depth map of by off-the-shelf Depth-Anything-V2 (you can download and install the corresponding enviroment from <a href="https://github.com/DepthAnything/Depth-Anything-V2" target="_blank">official</a>) and calibrate it:
```bash
python ./Data_preprocess/4_Depth_calibration_prox.py
```

Extract smplx-x parameters:
```bash
python ./Data_preprocess/5_Extract_smpl_parameters.py  
```

# Training
The total training scheme consists of the following two parts:
Scene radiance moding:
```bash
python 1_train_background_only_prox.py
```

Human radiance moding:
```bash
python 2_train_total_prox.py
```

# visualize
After training, you can run the following script to visualize the results:
```bash
python 3_visualize_reulst_prox.py
```


# Acknowledgement

This project is built on source codes from <a href="https://github.com/mikeqzy/3dgs-avatar-release" target="_blank">3DGS-Avatar</a>. We sincerely thank these authors for their awesome work.



